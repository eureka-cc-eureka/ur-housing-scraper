import asyncio
from playwright.async_api import async_playwright
import re
import requests
from datetime import datetime
import os
from dotenv import load_dotenv

load_dotenv()

# --- é…ç½® ---
NOTION_TOKEN = os.getenv("NOTION_TOKEN")
DATABASE_ID = os.getenv("DATABASE_D_ID")
AREAS = ["tokyo", "kanagawa", "chiba"]

HEADERS = {
    "Authorization": f"Bearer {NOTION_TOKEN}",
    "Content-Type": "application/json",
    "Notion-Version": "2022-06-28"
}

existing_pages_map = {}

def call_notion_api(method, url, data=None):
    try:
        if method == "POST":
            response = requests.post(url, headers=HEADERS, json=data)
        elif method == "PATCH":
            response = requests.patch(url, headers=HEADERS, json=data)

        if response.status_code not in [200, 201]:
            print(f"âŒ Notion API é”™è¯¯ ({response.status_code}): {response.text}")
            return None
        return response.json()
    except Exception as e:
        print(f"âŒ ç½‘ç»œè¯·æ±‚å¼‚å¸¸: {e}")
        return None

async def fetch_all_existing_pages():
    """ç¨‹åºå¯åŠ¨æ—¶ï¼Œä¸€æ¬¡æ€§è·å–æ•°æ®åº“æ‰€æœ‰æˆ¿æºçš„ URL å’Œä»·æ ¼"""
    global existing_pages_map
    print("ğŸ“¡ æ­£åœ¨åŒæ­¥ Notion æ•°æ®åº“ç°çŠ¶...")
    query_url = f"https://api.notion.com/v1/databases/{DATABASE_ID}/query"
    has_more = True
    next_cursor = None

    while has_more:
        payload = {"page_size": 100}
        if next_cursor:
            payload["start_cursor"] = next_cursor

        res = call_notion_api("POST", query_url, payload)
        if not res: break

        for page in res.get("results", []):
            name_list = page["properties"].get("å›¢åœ°åç§°", {}).get("title", [])
            name_text = name_list[0].get("plain_text", "æœªçŸ¥æˆ¿æº") if name_list else "æœªçŸ¥æˆ¿æº"
            url_prop = page["properties"].get("é“¾æ¥", {}).get("url")
            if url_prop:
                existing_pages_map[url_prop] = {
                    "page_id": page["id"],
                    "name": name_text,
                }
        has_more = res.get("has_more")
        next_cursor = res.get("next_cursor")

    print(f"âœ… åŒæ­¥å®Œæˆï¼Œåº“ä¸­ç°æœ‰ {len(existing_pages_map)} æ¡å›¢åœ°ã€‚")

async def scrape_danchi_details(page, danchi_url, seen_urls):
    danchi_name = "æœªçŸ¥å›¢åœ°"
    try:
        # è°ƒè¯•æ—¥å¿—ï¼šç¡®è®¤è¿›å…¥äº†å‡½æ•°
        seen_urls.add(danchi_url)
        
        # æ”¹ç”¨ networkidleï¼Œç¡®ä¿ç½‘ç»œè¯·æ±‚ç›¸å¯¹å®‰é™
        await page.goto(danchi_url, wait_until="commit", timeout=30000)
        await page.wait_for_selector("h1.article_headings", timeout=5000)
        try:
            # ä½¿ç”¨ JavaScript ç²¾å‡†æå– span é‡Œçš„æ–‡å­—ï¼Œå¿½ç•¥ rt æ³¨éŸ³
            danchi_name = await page.evaluate('''() => {
                const rubySpan = document.querySelector("h1.article_headings ruby span");
                const fallbackH1 = document.querySelector("h1.article_headings");
                if (rubySpan) return rubySpan.innerText.trim();
                if (fallbackH1) return fallbackH1.innerText.split('\\n')[0].trim();
                return "åç§°è§£æå¤±è´¥";
            }''')
        except Exception as e:
            print(f"    âš ï¸ åç§°æŠ“å–é‡è¯•ä¸­... {e}")

        print(f"    ğŸ˜ï¸ æŠ“å–åˆ°å›¢åœ°åç§°: {danchi_name}")
        async def get_coords(p):
            return await p.evaluate('''() => {
                const latEl = document.querySelector(".js-lat-data");
                const lngEl = document.querySelector(".js-lng-data");
                return latEl && lngEl ? { lat: latEl.value, lng: lngEl.value } : null;
            }''')
        
        map_url = danchi_url.replace(".html", "_map.html")
        print(f"    ğŸ”„ ä¸»é¡µæœªæ‰¾åˆ°åæ ‡ï¼Œå°è¯•è·³è½¬åœ°å›¾é¡µ: {map_url}")
        await page.goto(map_url, wait_until="domcontentloaded")
        # åœ¨åœ°å›¾é¡µç»™ä¸€ç‚¹ç¼“å†²æ—¶é—´
        await page.wait_for_timeout(1000)
        coords = await get_coords(page)
        if coords:
            lat_num = float(coords['lat'])
            lng_num = float(coords['lng'])
            print(f"    ğŸ“ åæ ‡æŠ“å–æˆåŠŸ: {lat_num}, {lng_num}")
        else:
            print(f"    âš ï¸ æœ€ç»ˆæœªèƒ½æ‰¾åˆ°åæ ‡æ ‡ç­¾")
            lat_num, lng_num = 0.0, 0.0

        
        props = {
            "å›¢åœ°åç§°": {"title": [{"text": {"content": danchi_name}}]},
            "çº¬åº¦": {"number": lat_num}, 
            "ç»åº¦": {"number": lng_num},
            "é“¾æ¥": {"url": danchi_url},
            "æ›´æ–°æ—¶é—´": {"date": {"start": datetime.now().strftime("%Y-%m-%dT%H:%M:%SZ")}}
        }
        
        # æ‰§è¡Œä¸Šä¼ 
        call_notion_api("POST", "https://api.notion.com/v1/pages", {"parent": {"database_id": DATABASE_ID}, "properties": props})
        print(f"    âœ¨ [æ–°å¢] {danchi_name} ({lat_num}, {lng_num})")
    except Exception as e:
        print(f"    âŒ æŠ“å–å¤±è´¥ {danchi_url}: {e}")

async def main():
    await fetch_all_existing_pages()
    seen_urls = set()

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=False)
        context = await browser.new_context()
        page = await context.new_page()

        for area_code in AREAS:
            print(f"\nğŸŒ æ­£åœ¨æ‰«æåœ°åŒº: {area_code.upper()}")
            # å¿…é¡»å…ˆç»è¿‡è¿™ä¸ªé¡µé¢å¹¶å‹¾é€‰ï¼Œå¦åˆ™ç›´æ¥è¿›å…¥ result å¯èƒ½ä¼šæ²¡æ•°æ®
            await page.goto(f"https://www.ur-net.go.jp/chintai/kanto/{area_code}/area/")
            await page.evaluate('document.querySelectorAll("input[type=\'checkbox\']").forEach(i => i.checked = true)')
            
            # ç‚¹å‡»æœç´¢æŒ‰é’®æˆ–ç›´æ¥è·³è½¬ç»“æœé¡µ
            await page.goto(f"https://www.ur-net.go.jp/chintai/kanto/{area_code}/result/")

            page_num = 1
            while True:
                print(f"--- ğŸ“„ {area_code.upper()} æ­£åœ¨æ‰«æç¬¬ {page_num} é¡µ ---")
                try:
                    await page.wait_for_selector("a.rep_bukken-link", timeout=10000)
                except:
                    print("  â„¹ï¸ è¯¥åœ°åŒºæ‰«æå®Œæ¯•æˆ–æœªå‘ç°æˆ¿æº")
                    break
                
                # è·å–å½“å‰é¡µæ‰€æœ‰é“¾æ¥
                links = [f"https://www.ur-net.go.jp{await el.get_attribute('href')}" 
                         for el in await page.query_selector_all("a.rep_bukken-link")]

                # å¤ç”¨åŒä¸€ä¸ªè¯¦æƒ…é¡µå¯¹è±¡ï¼Œé¿å…å¼€å¤ªå¤šçª—å£å¯¼è‡´ç”µè„‘å¡æ­»
                worker_page = await context.new_page()
                for link in links:
                    await scrape_danchi_details(worker_page, link, seen_urls)
                await worker_page.close()

                # ç¿»é¡µ
                next_btn = await page.query_selector("li.next a, a:has-text('æ¬¡ã¸')")
                if next_btn and await next_btn.is_visible():
                    page_num += 1
                    await next_btn.click()
                    await page.wait_for_timeout(4000)
                else: break

        await browser.close()
        print("\nğŸ‰ ä»»åŠ¡å…¨éƒ¨å®Œæˆï¼")

if __name__ == "__main__":
    asyncio.run(main())