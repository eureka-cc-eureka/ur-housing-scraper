import asyncio
from playwright.async_api import async_playwright
import re
import requests
from datetime import datetime
import os
from dotenv import load_dotenv

load_dotenv()

# --- é…ç½® ---
NOTION_TOKEN = os.getenv("NOTION_TOKEN")
DATABASE_ID = os.getenv("DATABASE_D_ID")
AREAS = ["tokyo", "kanagawa", "chiba"]

HEADERS = {
    "Authorization": f"Bearer {NOTION_TOKEN}",
    "Content-Type": "application/json",
    "Notion-Version": "2022-06-28"
}

existing_pages_map = {}

def call_notion_api(method, url, data=None):
    try:
        if method == "POST":
            response = requests.post(url, headers=HEADERS, json=data)
        elif method == "PATCH":
            response = requests.patch(url, headers=HEADERS, json=data)

        if response.status_code not in [200, 201]:
            print(f"âŒ Notion API é”™è¯¯ ({response.status_code}): {response.text}")
            return None
        return response.json()
    except Exception as e:
        print(f"âŒ ç½‘ç»œè¯·æ±‚å¼‚å¸¸: {e}")
        return None

async def fetch_all_existing_pages():
    """ç¨‹åºå¯åŠ¨æ—¶ï¼Œä¸€æ¬¡æ€§è·å–æ•°æ®åº“æ‰€æœ‰æˆ¿æºçš„ URL å’Œä»·æ ¼"""
    global existing_pages_map
    print("ğŸ“¡ æ­£åœ¨åŒæ­¥ Notion æ•°æ®åº“ç°çŠ¶...")
    query_url = f"https://api.notion.com/v1/databases/{DATABASE_ID}/query"
    has_more = True
    next_cursor = None

    while has_more:
        payload = {"page_size": 100}
        if next_cursor:
            payload["start_cursor"] = next_cursor

        res = call_notion_api("POST", query_url, payload)
        if not res: break

        for page in res.get("results", []):
            name_list = page["properties"].get("å›¢åœ°åç§°", {}).get("title", [])
            name_text = name_list[0].get("plain_text", "æœªçŸ¥æˆ¿æº") if name_list else "æœªçŸ¥æˆ¿æº"
            url_prop = page["properties"].get("é“¾æ¥", {}).get("url")
            if url_prop:
                existing_pages_map[url_prop] = {
                    "page_id": page["id"],
                    "name": name_text,
                }
        has_more = res.get("has_more")
        next_cursor = res.get("next_cursor")

    print(f"âœ… åŒæ­¥å®Œæˆï¼Œåº“ä¸­ç°æœ‰ {len(existing_pages_map)} æ¡å›¢åœ°ã€‚")

async def scrape_detail_page(page, url):
    page_info = existing_pages_map.get(url)
    if not page_info: return
    
    page_id = page_info["page_id"]
    name = page_info["name"]

    try:
        print(f"ğŸ§ æ­£åœ¨æŠ“å–: {name}")
        await page.goto(url, wait_until="domcontentloaded", timeout=60000)
        
        table_selector = "div.article_sliders_table"
        await page.wait_for_selector(table_selector, timeout=10000)

        rows = await page.query_selector_all(f"{table_selector} tr")
        
        data = {
            "price_min": None, "price_max": None, "common_fee": None,
            "room_min": None, "room_max": None,
            "area_min": None, "area_max": None
        }

        for row in rows:
            th = await row.query_selector("th")
            if not th: continue
            label = await th.inner_text()
            td = await row.query_selector("td")
            if not td: continue
            text = (await td.inner_text()).replace("\n", "").strip()

            # 1. è§£æä»·æ ¼å’Œå…±ç›Šè´¹
            if "å®¶è³ƒ" in label:
                prices = re.findall(r"([\d,]+)å††", text)
                if len(prices) >= 1:
                    data["price_min"] = prices[0].replace(",", "")
                    # å…œåº•ï¼šå¦‚æœæ²¡æœ‰ä¸Šé™ï¼Œå°±ç­‰äºä¸‹é™
                    data["price_max"] = prices[1].replace(",", "") if len(prices) >= 2 else data["price_min"]
                
                fee = re.search(r"\(([\d,]+)å††\)", text)
                if fee: data["common_fee"] = fee.group(1).replace(",", "")

            # 2. è§£æé—´å–å’Œé¢ç§¯
            elif "é–“å–ã‚Š/åºŠé¢ç©" in label:
                # åŒ¹é…å¦‚ 2LDK, 3DK
                rooms = re.findall(r"(\d[A-Z]+)", text)
                if len(rooms) >= 1:
                    data["room_min"] = rooms[0]
                    data["room_max"] = rooms[1] if len(rooms) >= 2 else data["room_min"]
                
                # åŒ¹é…å¦‚ 64, 80
                areas = re.findall(r"([\d.]+)ã¡", text)
                if len(areas) >= 1:
                    data["area_min"] = areas[0]
                    data["area_max"] = areas[1] if len(areas) >= 2 else data["area_min"]

        # --- æ„é€  Notion å±æ€§ (å¸¦å®‰å…¨æ£€æŸ¥) ---
        props = {}
        
        # æ•°å­—ç±»å‹è½¬æ¢ï¼šå¿…é¡»è½¬ä¸º intï¼Œä¸”ä¸èƒ½ä¸º None
        if data["price_min"]: props["ç§Ÿé‡‘ä¸‹é™"] = {"number": int(data["price_min"])}
        if data["price_max"]: props["ç§Ÿé‡‘ä¸Šé™"] = {"number": int(data["price_max"])}
        if data["common_fee"]: props["ç®¡ç†è´¹"] = {"number": int(data["common_fee"])}
        
        # æ–‡æœ¬ç±»å‹
        if data["area_min"]: props["é¢ç§¯ä¸‹é™"] = {"rich_text": [{"text": {"content": f"{data['area_min']}ã¡"}}]}
        if data["area_max"]: props["é¢ç§¯ä¸Šé™"] = {"rich_text": [{"text": {"content": f"{data['area_max']}ã¡"}}]}
        
        # Select ç±»å‹ (é€‰é¡¹å¿…é¡»æ˜¯å­—ç¬¦ä¸²)
        if data["room_min"]: props["æˆ¿å‹ä¸‹é™"] = {"select": {"name": data["room_min"]}}
        if data["room_max"]: props["æˆ¿å‹ä¸Šé™"] = {"select": {"name": data["room_max"]}}
        
        if props:
            call_notion_api("PATCH", f"https://api.notion.com/v1/pages/{page_id}", {"properties": props})
            print(f"âœ… æ›´æ–°æˆåŠŸ: {name}")

    except Exception as e:
        print(f"    âŒ æŠ“å–/æ›´æ–°å¤±è´¥ {url}: {e}")

# main å‡½æ•°ä¿æŒä½ æœ€åæä¾›çš„é‚£ä¸ªç‰ˆæœ¬å³å¯ï¼Œå®ƒå·²ç»æ˜¯åŸºäº Notion URL åˆ—è¡¨éå†çš„äº†ã€‚

async def main():
    # 1. ç¬¬ä¸€æ­¥ï¼šè·å– Notion æ•°æ®åº“ä¸­ç°æœ‰çš„æ‰€æœ‰é¡µé¢å’Œ URL
    await fetch_all_existing_pages()
    
    if not existing_pages_map:
        print("ç»ˆæ­¢ï¼šNotion æ•°æ®åº“ä¸­æ²¡æœ‰å‘ç°ä»»ä½•å¸¦æœ‰ URL çš„æ•°æ®ã€‚")
        return

    # 2. ç¬¬äºŒæ­¥ï¼šå¯åŠ¨æµè§ˆå™¨ï¼Œéå† URL è¿›è¡Œçˆ¬å–
    async with async_playwright() as p:
        # headless=True å»ºè®®æ­£å¼è¿è¡Œæ—¶å¼€å¯ï¼Œé€Ÿåº¦æ›´å¿«
        browser = await p.chromium.launch(headless=False) 
        context = await browser.new_context()
        page = await context.new_page()

        print(f"\nğŸš€ å¼€å§‹æ ¹æ® Notion åˆ—è¡¨æ›´æ–°è¯¦ç»†æ•°æ®ï¼Œå…± {len(existing_pages_map)} ä¸ªæˆ¿æº...")

        # ç›´æ¥éå†åŒæ­¥å›æ¥çš„ URL å­—å…¸
        for url in existing_pages_map.keys():
            await scrape_detail_page(page, url)
            # é€‚å½“å»¶è¿Ÿï¼Œé˜²æ­¢è¯·æ±‚è¿‡å¿«è¢«å°æˆ–è§¦å‘ Notion API é€Ÿç‡é™åˆ¶
            await asyncio.sleep(1)

        await browser.close()
        print("\nâœ¨ æ‰€æœ‰æˆ¿æºæ•°æ®æ›´æ–°ä»»åŠ¡å·²å®Œæˆï¼")

if __name__ == "__main__":
    asyncio.run(main())

